{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wRQLye8AVYLd",
    "outputId": "b3f4b632-3788-46ee-b3b0-182c99820198"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==1.11.0 in /Users/zhangyuqian/opt/anaconda3/lib/python3.9/site-packages (1.11.0)\r\n",
      "Requirement already satisfied: typing-extensions in /Users/zhangyuqian/opt/anaconda3/lib/python3.9/site-packages (from torch==1.11.0) (4.1.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==1.11.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HE-JA027Vx3v"
   },
   "source": [
    "# 1 Import package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ylws6sb1VkJG"
   },
   "outputs": [],
   "source": [
    "# import open method from io toolkit\n",
    "from io import open\n",
    "# for character normalization\n",
    "import unicodedata\n",
    "# for regular expressions\n",
    "import re\n",
    "# for random data generation\n",
    "import random\n",
    "# import torch toolkit for building network structures and functions\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# import toolkit of predefined optimization methods in torch\n",
    "from torch import optim\n",
    "# device selection, choose to run the code on cuda or cpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HFU7O_UDV43q"
   },
   "source": [
    "# 2 Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xCk-uRyHXhLH"
   },
   "source": [
    "# 2.1 Mapping words in a given language to values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "7EB5k3J9VsWG"
   },
   "outputs": [],
   "source": [
    "# start sign\n",
    "SOS_token = 0\n",
    "# end sign\n",
    "EOS_token = 1\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        \"\"\"The parameter 'name' in the initialization function represents the name of a language passed in\"\"\"\n",
    "        # pass the name into the class\n",
    "        self.name = name\n",
    "        # initialize the dictionary of words corresponding to natural values\n",
    "        self.word2index = {}\n",
    "        # initialize the dictionary of natural values corresponding to the vocabulary, where 0, 1 corresponding to SOS and EOS are already in it\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        # initialize the index of the natural number corresponding to the vocabulary, starting from 2 here, because 0, 1 are already occupied by the start and end flags\n",
    "        self.n_words = 2  \n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        \"\"\"Add a sentence function, which converts a sentence into a sequence of values, with the input parameter 'sentence' being a sentence\"\"\"\n",
    "        # segment the sentences according to the language characteristics of the country in general (the languages here are divided into words by spaces), and obtain the corresponding list of words\n",
    "        for word in sentence.split(' '):\n",
    "            # call addWord for processing\n",
    "            self.addWord(word)\n",
    "\n",
    "\n",
    "    def addWord(self, word):\n",
    "        \"\"\"Add a vocabulary function, which converts a vocabulary into a corresponding value, with the input parameter 'word' being a word\"\"\"\n",
    "        # determine if the word is already in the key of the self.word2index dictionary\n",
    "        if word not in self.word2index:\n",
    "            # if not, add this word to the dictionary, and assign a value to this word, i.e. self.n_words\n",
    "            self.word2index[word] = self.n_words\n",
    "            # add its inverted form to self.index2word simultaneously\n",
    "            self.index2word[self.n_words] = word\n",
    "            # if self.n_words is occupied, add 1 each time to turn it into a new self.n_words\n",
    "            self.n_words += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Instantiate parameters*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "hbJB8ABLWAUh"
   },
   "outputs": [],
   "source": [
    "name = \"eng\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Input Parameters*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "h3vgstZDWIkH"
   },
   "outputs": [],
   "source": [
    "sentence = \"hello I am Jay\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Call*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iVh1rGnlWLCe",
    "outputId": "9430a5e1-8954-40aa-b52a-808b608af11f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2index: {'hello': 2, 'I': 3, 'am': 4, 'Jay': 5}\n",
      "index2word: {0: 'SOS', 1: 'EOS', 2: 'hello', 3: 'I', 4: 'am', 5: 'Jay'}\n",
      "n_words: 6\n"
     ]
    }
   ],
   "source": [
    "engl = Lang(name)\n",
    "engl.addSentence(sentence)\n",
    "print(\"word2index:\", engl.word2index)\n",
    "print(\"index2word:\", engl.index2word)\n",
    "print(\"n_words:\", engl.n_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-ds5Fs-W6Qh"
   },
   "source": [
    "## 2.2 Normalizing String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "pT6gm7E6WOkv"
   },
   "outputs": [],
   "source": [
    "# converting unicode to Ascii (removing the accent marks in some languages: Ślusàrski)\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "\n",
    "def normalizeString(s):\n",
    "    \"\"\"String normalization function, parameter 's' represents the string passed in\"\"\"\n",
    "    # make characters lowercase and remove whitespace on both sides, then use unicodeToAscii to remove accent marks\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    # add a space before the .!?\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    # use regular expressions to replace all strings that are not upper and lower case letters and normal punctuation with spaces\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Input parameters*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "0v64hswnXBke"
   },
   "outputs": [],
   "source": [
    "s = \"Are you kidding me?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Call*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YFbe7KjLXE9q",
    "outputId": "3740186c-0b9f-4096-bdd5-f2ce1fc9fa61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "are you kidding me ?\n"
     ]
    }
   ],
   "source": [
    "nsr = normalizeString(s)\n",
    "print(nsr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YPguW-5rX6R2"
   },
   "source": [
    "## 2.3 Loading the data from the persistence file into memory, and instantiating the *Lang* class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "vvr6fSifXzCl"
   },
   "outputs": [],
   "source": [
    "data_path = '/Users/zhangyuqian/Desktop/ML project/sample_data:eng-fra.txt'\n",
    "\n",
    "def readLangs(lang1, lang2):\n",
    "    \"\"\"Read language function, parameter 'lang1' is the name of the source language, parameter 'lang2' is the name of the target language\n",
    "       Returns the corresponding class Lang object and a list of language pairs\"\"\"\n",
    "    # read language pairs from the file and store them in listlines with '/n' divisions\n",
    "    lines = open(data_path, encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "    # normalize the sentences in the lines list and divide them by '\\t' to form a sublist, i.e., a language pair\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines] \n",
    "    # pass the language name into the Lang class, get the corresponding language object, and return the result\n",
    "    input_lang = Lang(lang1)\n",
    "    output_lang = Lang(lang2)\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Input Parameters*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "SZtNx2SDY4Ew"
   },
   "outputs": [],
   "source": [
    "lang1 = \"eng\"\n",
    "lang2 = \"fra\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Call*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_oTuiK54Y6j1",
    "outputId": "828d27c8-da80-424f-f0d1-7b9b005b84cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_lang: <__main__.Lang object at 0x7fca07842280>\n",
      "output_lang: <__main__.Lang object at 0x7fca078422e0>\n",
      "The first five in pairs: [['go .', 'va !'], ['run !', 'cours !'], ['run !', 'courez !'], ['wow !', 'ca alors !'], ['fire !', 'au feu !']]\n"
     ]
    }
   ],
   "source": [
    "input_lang, output_lang, pairs = readLangs(lang1, lang2)\n",
    "print(\"input_lang:\", input_lang)\n",
    "print(\"output_lang:\", output_lang)\n",
    "print(\"The first five in pairs:\", pairs[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aqrSpPG7ZDme"
   },
   "source": [
    "## 2.4 Filtering out the language pairs that meet the requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ydUvEG3PZB6J"
   },
   "outputs": [],
   "source": [
    "# set the maximum number of words or punctuation in a sentence\n",
    "MAX_LENGTH = 10\n",
    "\n",
    "# select linguistic feature data with specified prefixes as training data\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "\n",
    "def filterPair(p):\n",
    "    \"\"\"Language pair filter function, the parameter 'p' represents the input language pair, such as ['she is afraid.', 'elle malade.']\"\"\"\n",
    "    # p[0] represents an English sentence, which should be divided by a length less than MAX_LENGTH and start with the specified prefix\n",
    "    # p[1] represents a French sentence, which should be divided in such a way that its length is less than MAX_LENGTH\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        p[0].startswith(eng_prefixes) and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH \n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    \"\"\"Filter on a list of language pairs, the parameter 'pairs' represents a list of language pairs, abbreviated as language pair list\"\"\"\n",
    "    # function directly traverses each language pair in the list and calls filterPair\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Input parameters*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "nhEkMfc5ZMNP"
   },
   "outputs": [],
   "source": [
    "# here, use the output of the readLangs function pairs as the input parameter pairs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Call*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_peJTJJ8ZeJa",
    "outputId": "b087b063-9f9a-48b5-9bcc-7ca1a1de84e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first five in filtered pairs: [['i m .', 'j ai ans .'], ['i m ok .', 'je vais bien .'], ['i m ok .', 'ca va .'], ['i m fat .', 'je suis gras .'], ['i m fat .', 'je suis gros .']]\n"
     ]
    }
   ],
   "source": [
    "fpairs = filterPairs(pairs)\n",
    "print(\"The first five in filtered pairs:\", fpairs[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y4gwAJFCZmKk"
   },
   "source": [
    "## 2.5 Integrating the data preparation functions shown above, and mapping the language pairs to values using the *Lang* class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "GngVZouwZhlt"
   },
   "outputs": [],
   "source": [
    "def prepareData(lang1, lang2):\n",
    "    \"\"\"Data preparation function, which completes the mapping of all string data to numeric data and the filtering of language pairs\n",
    "       The parameters 'lang1', 'lang2' represent the names of the source and target languages respectively\"\"\"\n",
    "    # first, obtain the input_lang, output_lang objects and a list of language pairs of the string type through the readLangs function\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2)\n",
    "    # filter lists for string type languages\n",
    "    pairs = filterPairs(pairs)\n",
    "    # traverse the list for the filtered language\n",
    "    for pair in pairs:\n",
    "        # use the addSentence methods of input_lang and output_lang for numerical mapping\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    # return the value mapped object and the filtered language pair\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Call*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WvGccyTzaLJb",
    "outputId": "0148a096-36eb-4dc7-8850-ef4cbaa3e377"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_n_words: 2803\n",
      "output_n_words: 4345\n",
      "['i m impatient .', 'je suis impatient .']\n"
     ]
    }
   ],
   "source": [
    "input_lang, output_lang, pairs = prepareData('eng', 'fra')\n",
    "print(\"input_n_words:\", input_lang.n_words)\n",
    "print(\"output_n_words:\", output_lang.n_words)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9iq3cW6JaT9l"
   },
   "source": [
    "## 2.6 Transform the language pairs into the tensor needed for model input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "cT1Fxt03aSwo"
   },
   "outputs": [],
   "source": [
    "def tensorFromSentence(lang, sentence):\n",
    "    \"\"\"Convert a text sentence to a tensor, the parameter 'lang' represents the instantiated object of the Lang passed in, sentence is the pre-converted sentence\"\"\"\n",
    "    # segment the sentence and traverse each word, then use word2index method of 'lang' to find its corresponding index\n",
    "    # we can obtain the list of values corresponding to the sentence\n",
    "    indexes = [lang.word2index[word] for word in sentence.split(' ')]\n",
    "    # add the end-of-sentence marker\n",
    "    indexes.append(EOS_token)\n",
    "    # wrap it as a tensor using torch.tensor, and change its shape to nx1 to facilitate subsequent calculations\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    \"\"\"Convert a language pair to a tensor pair, with parameter 'pair' as a language pair\"\"\"\n",
    "    # call tensorFromSentence to process the source language and the target language separately to obtain the corresponding tensor representation\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    # return the tuple\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Input parameters*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "U9rePIoaaeoP"
   },
   "outputs": [],
   "source": [
    "# the first entry of pairs\n",
    "pair = pairs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Call*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o_pLKI2VaiVf",
    "outputId": "571c48a5-6ed4-42a1-86ef-b716f1b4c910"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[2],\n",
      "        [3],\n",
      "        [4],\n",
      "        [1]]), tensor([[2],\n",
      "        [3],\n",
      "        [4],\n",
      "        [5],\n",
      "        [1]]))\n"
     ]
    }
   ],
   "source": [
    "pair_tensor = tensorsFromPair(pair)\n",
    "print(pair_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U338BBXxamz6"
   },
   "source": [
    "# 3 Building GRU-based encoders and decoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "isY5rXMWavdA"
   },
   "source": [
    "## 3.1 Building a GRU-based encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "BOLEBGEsamWf"
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        \"\"\"There are two initialization parameters: 'input_size' represents the input size of the decoder, \n",
    "        i.e. the word list size of the source language; 'hidden_size' represents the number of hidden \n",
    "        nodes of GRU, which also represents the word embedding dimension, and is also the input size of GRU.\"\"\"\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        # pass the parameter 'hidden_size' into the class\n",
    "        self.hidden_size = hidden_size\n",
    "        # instantiate the predefined Embedding layer in nn, whose parameters are input_size, hidden_size\n",
    "        # the word embedding dimension here is hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        # instantiate the predefined GRU layer in nn, whose parameter is hidden_size\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        \"\"\"There are two parameters in the encoder forward logic function: 'input' represents the input tensor \n",
    "        of the Embedding layer of the source language; hidden represents the initial hidden layer tensor of the \n",
    "        encoder layer gru\"\"\"\n",
    "        # embed the input tensor, and make its shape (1, 1, -1), -1 means the dimension is calculated automatically\n",
    "        # theoretically, the encoder takes only one word at a time as input, so the size of the mapped vocabulary should be [1, embedding]\n",
    "        # the reason for the conversion to 3D is that the predefined gru in the torch must use a 3D tensor as input, so expand it by one dimension\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        # then, pass in the output of the embedding layer and the initial hidden as input to gru \n",
    "        # get the output of the final gru and the corresponding hidden tensor hidden, and return the result\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        \"\"\"Initialize the hidden tensor function\"\"\"\n",
    "        # initialize the hidden tensor to a 0 tensor of size 1 * 1 * self.hidden_size\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Instantiate parameters*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "s1tm8Ojha3Hg"
   },
   "outputs": [],
   "source": [
    "hidden_size = 25\n",
    "input_size = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Input parameters*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "vxeTUZBpa6q9"
   },
   "outputs": [],
   "source": [
    "# pair_tensor[0] represents the sentence in the source language, i.e. English, and pair_tensor[0][0] represents the first word in the sentence\n",
    "input = pair_tensor[0][0]\n",
    "# initialize the first hidden tensor, a 0 tensor of 1 * 1 * hidden_size\n",
    "hidden = torch.zeros(1, 1, hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Call*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DZcQOgQWa-Av",
    "outputId": "d369511c-cba5-4314-f990-d64433f50472"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.2365, -0.4685,  0.3727,  0.2041,  0.5849,  0.3269, -0.4125,\n",
      "          -0.2622,  0.4215,  0.2241, -0.3261,  0.4786,  0.3804, -0.4119,\n",
      "           0.0825, -0.2612, -0.5668, -0.0293,  0.1895, -0.0863, -0.2056,\n",
      "          -0.3240, -0.3006, -0.0553,  0.0500]]], grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "encoder = EncoderRNN(input_size, hidden_size)\n",
    "encoder_output, hidden = encoder(input, hidden)\n",
    "print(encoder_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KkekqwG7bIFq"
   },
   "source": [
    "## 3.2 Building a GRU-based decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "QYKewFN3bHLL"
   },
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        \"\"\"There are two parameters in the initialization function, \n",
    "        'hidden_size' represents the input size of GRU in the decoder and also the number of its hidden layer nodes\n",
    "        'output_size' represents the output size of the whole decoder, which is also the size we want to get, i.e. the word list size of the target language.\"\"\"\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        # pass the hidden_size into the class\n",
    "        self.hidden_size = hidden_size\n",
    "        # instantiate an Embedding layer object in nn, whose parameter output here represents the word list size of the target language\n",
    "        # 'hidden_size' indicates the word embedding dimension of the target language\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        # instantiate the GRU object with the input parameter hidden_size, which means its input size is the same as the number of hidden nodes\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        # instantiate a linear layer, making linear changes to the output of GRU\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        # process with softmax for classification\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        \"\"\"There are two parameters in the forward logic function of the decoder: \n",
    "        'input' represents the input tensor of the Embedding layer of the target language\n",
    "        'hidden' represents the initial hidden layer tensor of the decoder GRU\"\"\"\n",
    "        # embed the input tensor, and make its shape (1, 1, -1). -1 means the dimension is calculated automatically\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        # process the output using the relu function to make the Embedding matrix more sparse to prevent overfitting\n",
    "        output = F.relu(output)\n",
    "        # pass the output of the embedding and the initialized hidden tensor to the decoder gru\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        # it can be downscaled by output[0], for the output of GRU is also a three-dimensional tensor where the first dimension is meaningless\n",
    "        # pass it to the linear layer for transformation, and process it with softmax for classification.\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        \"\"\"Initialize the hidden tensor function\"\"\"\n",
    "        # initialize the hidden tensor to a 0 tensor of size 1 * 1 * self.hidden_size\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Instantiate parameters*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "-bMWOHXEbPzP"
   },
   "outputs": [],
   "source": [
    "hidden_size = 25\n",
    "output_size = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Input parameters*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "WutcoQeqbTGL"
   },
   "outputs": [],
   "source": [
    "# pair_tensor[1] represents the sentence in the target language, i.e. French, and pair_tensor[1][0] represents the first word in the sentence\n",
    "input = pair_tensor[1][0]\n",
    "# Initialize the first hidden tensor, a 0 tensor of 1 * 1 * hidden_size\n",
    "hidden = torch.zeros(1, 1, hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Call*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6InkitzxbX6f",
    "outputId": "54051f09-9e69-4f2f-ba1e-ff80e6e49fa7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.1861, -2.3541, -2.5320, -2.2448, -2.4298, -2.2694, -2.2059, -2.4538,\n",
      "         -2.0324, -2.4228]], grad_fn=<LogSoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "decoder = DecoderRNN(hidden_size, output_size)\n",
    "output, hidden = decoder(input, hidden)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37PzBYcDbds5"
   },
   "source": [
    "## 3.3 Building decoder based on GRU and Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "uh85hfJ4bcmA"
   },
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        \"\"\"There are 4 parameters in the initialization function:\n",
    "        'hidden_size' represents the input size of GRU in the decoder and also the number of its hidden layer nodes\n",
    "        'output_size' represents the output size of the whole decoder, i.e. the word list size of the target language.\n",
    "        'dropout_p' represents the zeroing ratio when we use the dropout layer, default 0.1, max_length represents the maximum length of the sentence\"\"\"\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # instantiate an Embedding layer, with input parameters self.output_size and self.hidden_size\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        # according to the QKV theory of attention, the input parameters of attention are three Q, K, V\n",
    "        # attention weights are calculated using Q and K to obtain the weight matrix, and then matrix multiplication is done with V to obtain the attentional representation of V.\n",
    "        # stitch Q and K together on the vertical axis, perform the linear variation once, obtain the result by softmax processing, and make a tensor multiplication with V\n",
    "        \n",
    "        # need a matrix for linear transformation, instantiate nn.Linear\n",
    "        # since its input is a splice of Q, K, the first parameter is self.hidden_size * 2, and the second parameter is self.max_length\n",
    "        # Here, Q is the output of the Embedding layer of the decoder, K is the hidden layer output of the GRU of the decoder\n",
    "        # V is the output of the encoder layer\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        # Next, instantiate another linear layer, which is used to normalize the output size\n",
    "        # the input dimension is self.hidden_size * 2\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        # Next, instantiate a nn.Dropout layer and pass in self.dropout_p\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        # then, instantiate nn.GRU of which input and hidden layer size are both self.hidden_size\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        # Finally instantiate the linear layer behind gru, which is the decoder output layer.\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        \"\"\"There are 3 input parameters in the forward function: \n",
    "        the source data input tensor, the initial hidden tensor, and the output tensor of the decoder\"\"\"\n",
    "\n",
    "        # subject the input tensor to Embedding layers, and expand dimensionality\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        # use dropout for random discarding to prevent overfitting\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        # stitch Q, K on the vertical axis, make a linear change, and use softmax processing.\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "\n",
    "        # perform the bmm operation\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        # reduce the dimensionality by taking [0], and splice Q with the calculation result of the previous step again\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "\n",
    "        # use the linear layer action to perform a linear transformation on the result of the previous step, and extend the dimensionality\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        # activate with relu\n",
    "        output = F.relu(output)\n",
    "\n",
    "        # pass in the result after the activation as input to gru along with hidden\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        # downscale the results, and process them with softmax\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        # return the decoder result, the final hidden layer tensor, and the attention weight tensor\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        \"\"\"Initialize the hidden tensor function\"\"\"\n",
    "        # initialize the hidden tensor to a 0 tensor of size 1 * 1 * self.hidden_size\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Instantiate parameters*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "Z9ACrvaGbtl8"
   },
   "outputs": [],
   "source": [
    "hidden_size = 25\n",
    "output_size = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Input parameters*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "QaADBOLvbv5W"
   },
   "outputs": [],
   "source": [
    "input = pair_tensor[1][0]\n",
    "hidden = torch.zeros(1, 1, hidden_size)\n",
    "# encoder_outputs needs to be a stack of outputs for each time step in the encoder\n",
    "# its shape should be 10x25, so directly initialize a tensor here at random\n",
    "encoder_outputs  = torch.randn(10, 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Call*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qbRhKZtcbyO0",
    "outputId": "d3d938e4-c0f5-4207-9c11-5dcbf9bf9384"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.1850, -2.0518, -2.1782, -2.4231, -2.2649, -2.3744, -2.4032, -2.4213,\n",
      "         -2.4086, -2.3964]], grad_fn=<LogSoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "decoder = AttnDecoderRNN(hidden_size, output_size)\n",
    "output, hidden, attn_weights= decoder(input, hidden, encoder_outputs)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gp5DRV43cYyr"
   },
   "source": [
    "# 4 Construct model training function and perform training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C4N0d46KcihD"
   },
   "source": [
    "## 4.1 Constructing training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "O-nsWC4GcdIy"
   },
   "outputs": [],
   "source": [
    "# set the teacher_forcing ratio to 0.5\n",
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    \"\"\"There are 8 input parameters in the training function: \n",
    "    input_tensor: input tensor of the source language,\n",
    "    target_tensor: target language input tensor, and\n",
    "    encoder, decoder: encoder and decoder instantiation objects,\n",
    "    encoder_optimizer, decoder_optimizer: encoder and decoder optimization methods, \n",
    "    criterion: loss function calculation method, and\n",
    "    max_length: the maximum length of the sentence\"\"\"\n",
    "\n",
    "    # initialize the hidden tensor\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    # encoder and decoder optimizer gradient to 0\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    # obtain the corresponding lengths based on the source and target text tensor\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    # initialize the encoder output tensor in the shape of a 0-tensor of max_length * encoder.hidden_size\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    # initialize set loss to 0\n",
    "    loss = 0\n",
    "\n",
    "    # traverse the input tensor index\n",
    "    for ei in range(input_length):\n",
    "        # extract the tensor representation of the corresponding word from the input_tensor according to the index \n",
    "        # and pass it into the encoder object together with the initialized hidden tensor\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        # store the output of encoder_output (3D tensor) and its vectorized form into encoder_outputs\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    # initialize the first input of the decoder, i.e. start sign\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    # initialize the implicit tensor of the decoder, i.e. the implicit output of the encoder\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    # determine whether to use teacher_forcing based on the random number compared to teacher_forcing_ratio\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    # if use teacher_forcing\n",
    "    if use_teacher_forcing:\n",
    "        # traverse the target tensor index\n",
    "        for di in range(target_length):\n",
    "            # pass decoder_input, decoder_hidden, encoder_outputs (Q, K, V in attention) into decoder to obtain decoder_output, decoder_hidden, decoder_attention\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            # due to the use of teacher_forcing, we only use target_tensor[di] to calculate the loss regardless of the decoder_output of the decoder\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            # meanwhile, force the next decoder input to be set to 'the correct answer'\n",
    "            decoder_input = target_tensor[di]  \n",
    "\n",
    "    else:\n",
    "        # if not use teacher_forcing\n",
    "        # still traverse the target tensor index\n",
    "        for di in range(target_length):\n",
    "            # pass decoder_input, decoder_hidden, encoder_outputs into decoder to obtain decoder_output, decoder_hidden, decoder_attention\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            # here, get the answer from decoder_output\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            # still use decoder_output and target_tensor[di] as the loss calculation \n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            # if the output value is a terminator, the loop stops\n",
    "            if topi.squeeze().item() == EOS_token:\n",
    "                break\n",
    "            # otherwise, descend the topi and separate the value to decoder_input for the next operation\n",
    "            # the detachment of the detachment here makes this decoder_input independent of the tensor map constructed by the model, \n",
    "            # equivalent to a completely new external input\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "\n",
    "    # back propagation of errors\n",
    "    loss.backward()\n",
    "    # optimize encoders and decoders are optimized i.e. update parameters\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    # return average loss\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PDoqq4uLcr4h"
   },
   "source": [
    "## 4.2 Constructing time calculation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "a9lT4ViScqvE"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "def timeSince(since):\n",
    "    \"Get the training elapsed time per print, 'since' represents the training start time\"\n",
    "    # get current time\n",
    "    now = time.time()\n",
    "    # get the time difference (training time consuming)\n",
    "    s = now - since\n",
    "    # convert seconds to minutes, and round to the nearest whole number\n",
    "    m = math.floor(s / 60)\n",
    "    # calculate the number of seconds left to make up the 1 minute\n",
    "    s -= m * 60\n",
    "    # returns the elapsed time for the specified format\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Input parameters*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "8HV-TV3PcyOU"
   },
   "outputs": [],
   "source": [
    "# assume that the model training starts 10min before\n",
    "since = time.time() - 10*60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Call*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0WwHEWjDc0Sg",
    "outputId": "8d78d899-2e2e-4d09-d930-c05201300a01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10m 0s\n"
     ]
    }
   ],
   "source": [
    "period = timeSince(since)\n",
    "print(period)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jKcUTvm1c7lC"
   },
   "source": [
    "## 4.3 Calling training functions and printing logs and graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "HXm8OP_Tc6Ym"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    \"\"\"There are 6 input parameters in the training iteration function, namely:\n",
    "    encoder, decoder: encoder and decoder objects.\n",
    "    n_iters: the total number of iterative steps, \n",
    "    print_every: print log interval, \n",
    "    plot_every: interval to plot the loss curve, \n",
    "    learning_rate\"\"\"\n",
    "    # get training start time stamp\n",
    "    start = time.time()\n",
    "    # average loss saving list for each loss interval for plotting loss curves\n",
    "    plot_losses = []\n",
    "\n",
    "    # total loss per print log interval, initially 0\n",
    "    print_loss_total = 0  \n",
    "    # total loss for each plotted loss interval, initially 0\n",
    "    plot_loss_total = 0  \n",
    "\n",
    "    # use the predefined SGD as an optimizer, pass the parameters and learning rate into it\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "    # select the loss function\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    # loop according to set iteration step\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        # randomly select from the list of language pairs as a training statement one at a time\n",
    "        training_pair = tensorsFromPair(random.choice(pairs))\n",
    "        # take the input tensor and target tensor from training_pair respectively\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        # obtain the loss of the model run by the train function\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        # sum up losses\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        # when the iteration step reaches the log printing interval\n",
    "        if iter % print_every == 0:\n",
    "            # obtain average loss through dividing the total loss by the interval\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            # reset the total loss to zero\n",
    "            print_loss_total = 0\n",
    "            # print the logs with the following contents: \n",
    "            # training time, current iteration step, current progress percentage, current average loss\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        # when the iteration step reaches the loss drawing interval\n",
    "        if iter % plot_every == 0:\n",
    "            # obtain Average loss through dividing the total loss by the interval\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            # load the average loss into the plot_losses list\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            # reset total loss to 0\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    # plot loss curves\n",
    "    plt.figure()  \n",
    "    plt.plot(plot_losses)\n",
    "    # save to specified path\n",
    "    plt.savefig(\"./s2s_loss.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Input parameters*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "GB6KpH64dDXT"
   },
   "outputs": [],
   "source": [
    "# set the hidden layer size to 256, which is also the word embedding dimension      \n",
    "hidden_size = 256\n",
    "# get the total number of input words by input_lang.n_words and pass it to EncoderRNN class together with hidden_size\n",
    "# get encoder object encoder1\n",
    "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "\n",
    "# get the total number of target words by output_lang.n_words and pass it into AttnDecoderRNN class together with hidden_size and dropout_p\n",
    "# get decoder object attn_decoder1\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
    "\n",
    "# set the number of iteration steps \n",
    "n_iters = 75000\n",
    "# set the log printing interval\n",
    "print_every = 5000 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Call*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 537
    },
    "id": "7XG9PBfndFk9",
    "outputId": "71c57dbb-39a7-4536-dba0-5d63d46f1d16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3m 36s (5000 6%) 3.4072\n",
      "7m 18s (10000 13%) 2.7983\n",
      "10m 58s (15000 20%) 2.4237\n",
      "14m 39s (20000 26%) 2.1756\n",
      "18m 21s (25000 33%) 1.9500\n",
      "22m 7s (30000 40%) 1.8002\n",
      "25m 53s (35000 46%) 1.6398\n",
      "29m 35s (40000 53%) 1.5193\n",
      "33m 24s (45000 60%) 1.4244\n",
      "37m 12s (50000 66%) 1.2928\n",
      "40m 54s (55000 73%) 1.2234\n",
      "44m 40s (60000 80%) 1.1583\n",
      "48m 23s (65000 86%) 1.0771\n",
      "52m 8s (70000 93%) 1.0315\n",
      "55m 52s (75000 100%) 0.9732\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# call trainIters for model training, pass encoder object encoder1, coder object attn_decoder1, number of iteration steps, log printing interval into it\n",
    "trainIters(encoder1, attn_decoder1, n_iters, print_every=print_every)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "twIrCKlDeZA_"
   },
   "source": [
    "# 5 Construct model evaluation functions and perform testing and Attention effect analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ZuIzLbTebvc"
   },
   "source": [
    "## 5.1 Constructing model evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "FYVmNuDsef1s"
   },
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    \"\"\"There are 4 input parameters in the evaluation function, which are:\n",
    "    encoder, decoder: encoder and decoder objects,\n",
    "    sentence: the sentence to be evaluated.\n",
    "    max_length: the maximum length of the sentence\"\"\"\n",
    "\n",
    "    # no gradient calculation in the evaluation phase\n",
    "    with torch.no_grad():\n",
    "        # tensor representation of the input sentences\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        # get the input sentence length\n",
    "        input_length = input_tensor.size()[0]\n",
    "        # initialize the encoder hidden tensor\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        # initialize the encoder output tensor to be a 0-tensor of max_lengthxencoder.hidden_size\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        # iterate through the input tensor index\n",
    "        for ei in range(input_length):\n",
    "             # extract the tensor representation of the corresponding word from the input_tensor according to the index \n",
    "             # and pass it into the encoder object together with the initialized hidden tensor\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            # store the output of encoder_output (3D tensor) and its vectorized form into encoder_outputs\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        # initialize the first input of the decoder, i.e. the start sign\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device) \n",
    "        # initialize the implicit tensor of the decoder, i.e. the implicit output of the encoder\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        # initialize the list of predicted words\n",
    "        decoded_words = []\n",
    "        # initialize the attention tensor\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "        # start loop decoding\n",
    "        for di in range(max_length):\n",
    "            # pass decoder_input, decoder_hidden, encoder_outputs into decoder to obtain decoder_output, decoder_hidden, decoder_attention\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "\n",
    "            # take all the attention results and store them in the initialized attention tensor\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            # obtain the value with the highest probability and its index object from the decoder output\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            # take the value from the index object and compare it with the end flag value\n",
    "            if topi.item() == EOS_token:\n",
    "                # if it is the end flag value, load the end flag into the decoded_words list (the end of translation)\n",
    "                decoded_words.append('<EOS>')\n",
    "                # exit loop\n",
    "                break\n",
    "\n",
    "            else:\n",
    "                # otherwise, find its corresponding word in the index2word dictionary of the output language according to the index and load it into decoded_words\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "            # deal the index of this prediction by dimensionality reduction, and ssign it to the decoder_input for the next prediction\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "        # return the result decoded_words and the full attention tensor, and cut out the unused parts\n",
    "        return decoded_words, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Randomly select a specified amount of data for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "4EkzHr-Weof1"
   },
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=6):\n",
    "    \"\"\"Randomly evaluate function, input parameters 'encoder' and 'decoder' represents the encoder and decoder objects, \n",
    "    n represents the number of tests\"\"\"\n",
    "    # loop for test numbers\n",
    "    for i in range(n):\n",
    "        # randomly select language pairs from pairs\n",
    "        pair = random.choice(pairs)\n",
    "        # > represents input\n",
    "        print('>', pair[0])\n",
    "        # = represents the correct output\n",
    "        print('=', pair[1])\n",
    "        # call evaluate for prediction\n",
    "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
    "        # connect the results into sentences\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        # < represents the output of the model\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Call*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IMeCbgDAerRs",
    "outputId": "872e191e-6413-4bb3-a651-701d8713f786"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> i m not even a little hungry .\n",
      "= je n ai pas faim du tout .\n",
      "< je n ai pas faim de faim . <EOS>\n",
      "\n",
      "> he is a lazy fellow .\n",
      "= c est un type paresseux .\n",
      "< c est une femme paresseux . <EOS>\n",
      "\n",
      "> i m feeling much better today .\n",
      "= je me sens beaucoup mieux aujourd hui .\n",
      "< je me sens beaucoup mieux aujourd hui . <EOS>\n",
      "\n",
      "> i am afraid to go .\n",
      "= j ai peur d y aller .\n",
      "< j ai peur de m y rendre . <EOS>\n",
      "\n",
      "> i am paid weekly .\n",
      "= je suis payee a la semaine .\n",
      "< je suis en le . <EOS>\n",
      "\n",
      "> we re not getting any younger .\n",
      "= nous ne rajeunissons pas .\n",
      "< nous ne rajeunissons pas . <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# call evaluateRandomly for model testing\n",
    "evaluateRandomly(encoder1, attn_decoder1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P5iI8L1CezdY"
   },
   "source": [
    "## 5.3 Visualizing Attention tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "id": "_VOKvGr8etga",
    "outputId": "dd43ab61-f50f-4876-ce33-db3205a731f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nous', 'sommes', 'tous', 'deux', 'enseignants', '.', '<EOS>']\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 411.429x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentence = \"we re both teachers .\"\n",
    "# call the evaluation function\n",
    "output_words, attentions = evaluate(encoder1, attn_decoder1, sentence)\n",
    "print(output_words)\n",
    "# convert attention tensor to numpy, use matshow to plot\n",
    "plt.matshow(attentions.numpy())\n",
    "# save image\n",
    "plt.savefig(\"./s2s_attn.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "nlp_machine tralation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
